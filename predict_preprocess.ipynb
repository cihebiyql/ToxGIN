{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from biopandas.pdb import PandasPdb\n",
    "from biopandas.mmcif import PandasMmcif\n",
    "import torch\n",
    "import dgl\n",
    "import tokenizers\n",
    "import transformers\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aaindex1(file_path):\n",
    "    aaindex1_df = pd.read_csv(file_path, index_col='Description')\n",
    "    aaindex_dict = {aa: aaindex1_df[aa].values for aa in aaindex1_df.columns}\n",
    "    return aaindex_dict\n",
    "\n",
    "def extract_features(sequence, aaindex_dict):\n",
    "    features = []\n",
    "    for aa in sequence:\n",
    "        if aa in aaindex_dict:\n",
    "            features.append(aaindex_dict[aa])\n",
    "        else:\n",
    "            features.append(np.full((len(next(iter(aaindex_dict.values()))),), np.nan))\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_graph(coords, threshold=8.0):\n",
    "    all_diffs = np.expand_dims(coords, axis=1) - np.expand_dims(coords, axis=0)\n",
    "    distance = np.sqrt(np.sum(np.power(all_diffs, 2), axis=-1))\n",
    "    adj = distance < threshold\n",
    "    u, v = np.nonzero(adj)\n",
    "    u, v = torch.from_numpy(u), torch.from_numpy(v)\n",
    "    graph = dgl.graph((u, v), num_nodes=coords.shape[0])\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses ESM2_t36_3B_UR50D as a feature extractor and can be downloaded from HuggingFace:\n",
    "https://huggingface.co/facebook/esm2_t36_3B_UR50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ciheb\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6abf364dd946c3b8976e4d21108ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ciheb\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = \"facebook/esm2_t36_3B_UR50D\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "config.hidden_dropout = 0.\n",
    "config.hidden_dropout_prob = 0.\n",
    "config.attention_dropout = 0.\n",
    "config.attention_probs_dropout_prob = 0.\n",
    "encoder = AutoModel.from_pretrained(model, config=config).to(device).eval()\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_encode(seq):\n",
    "    spaced_seq = \" \".join(list(seq))\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        spaced_seq, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True,\n",
    "        max_length=60,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    last_hidden_states = outputs[0]\n",
    "    encoded_seq = last_hidden_states[inputs['attention_mask'].bool()][1:-1]\n",
    "    return encoded_seq\n",
    "\n",
    "def aggre(s):\n",
    "    if type(s.values[0]) == str:\n",
    "        return s.values[0]\n",
    "    return np.mean(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the CSV file path for writing sequences and the path for storing structure files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_map = {'VAL': 'V', 'PRO': 'P', 'ASN': 'N', 'GLU': 'E', 'ASP': 'D', 'ALA': 'A', 'THR': 'T', 'SER': 'S',\n",
    "          'LEU': 'L', 'LYS': 'K', 'GLY': 'G', 'GLN': 'Q', 'ILE': 'I', 'PHE': 'F', 'CYS': 'C', 'TRP': 'W',\n",
    "          'ARG': 'R', 'TYR': 'Y', 'HIS': 'H', 'MET': 'M'}\n",
    "\n",
    "df = pd.read_csv('test_sequence.csv')  # Path to store sequence files here.\n",
    "root_dir = 'test_dataset' # Path to store the generated dataset here.\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "graphs = []\n",
    "pdb = None\n",
    "\n",
    "aaindex_dict = load_aaindex1('aaindex1.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for index,row in df.iterrows():    \n",
    "    atom_df = PandasPdb().read_pdb(f'test_structures/{row.sequence}.pdb') # Path to store structure files here.\n",
    "    atom_df = atom_df.df['ATOM']\n",
    "    residue_df = atom_df.groupby('residue_number', as_index=False).agg(aggre).sort_values('residue_number')\n",
    "    del atom_df\n",
    "    residue_df['letter'] = residue_df.residue_name.map(aa_map)\n",
    "    \n",
    "    graph = None\n",
    "    pdb_seq = ''.join(residue_df.letter.values)\n",
    "\n",
    "    if graph is None:\n",
    "        graph = generate_graph(residue_df.loc[:, ['x_coord', 'y_coord', 'z_coord']].values)\n",
    "        graphs.append(graph)\n",
    "        encoded_pdb_seq = seq_encode(pdb_seq).cpu().numpy()\n",
    "        features = extract_features(pdb_seq, aaindex_dict)\n",
    "        normalized_features = scaler.fit_transform(features.astype(np.float32))\n",
    "        encoded_pdb_seq = np.concatenate([encoded_pdb_seq, normalized_features], axis=-1)\n",
    "        path = f'{row.sequence}'\n",
    "        np.savez_compressed(root_dir+'/'+path, wildtype_seq=encoded_pdb_seq)\n",
    "dgl.save_graphs(root_dir+'/dgl_graph.bin', graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
